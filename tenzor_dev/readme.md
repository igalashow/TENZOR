Здесь содержится решение тестового задания на вакансию Junior Python-разработчика в компанию "Тензор" (май 2021).

### Описание алгоритма
В целом задание можно разделить на две независимые подзадачи разной сложности:
1) выявление полезного текстового контента на веб-странице
2) форматирование текста в выходном файле

#### 1)
Выявление полезного контента - это востребованная сложная задача, решаемая в настоящий момент с применением эвристического анализа, а также технологий AI.
Существуют open-source [[1]](https://github.com/chrisspen/webarticle2text) [[2]](https://github.com/codelucas/newspaper), а также коммерческие решения [[3]](https://www.diffbot.com/products/extract/) этой задачи, находящиеся явно за пределами компетенции претендента на Junior-позицию. (Есть, кстати, даже [сравнение](https://github.com/scrapinghub/article-extraction-benchmark) разных библиотек).

Предложенное здесь упрощенное решение заключается в следующем:
- Алгоритм выявляет на веб-странице теги абзацев \<p>
- Определется родительский тег для каждого \<p>.
- Определяется родителький тег, содержащий максимальное количество абзацев текста.

Таким образом с некоторой долей вероятности определяется локация основного текстового контента 
на веб-странице, а элементы, содержащие незначительное количество текста (например, комменты), отсекаются.

**Недостаток алгоритма:** работает только для "классической" вёрстки, когда текст статьи помещается в теги \<p>.

**Решение:** проапгрейдиться до Senior, написать свою библиотеку с блэкджеком и эвристикой.

#### 2)
Форматирование текста реализовано в целом по следующей схеме:
- Абзацы и заголовки отбиваются пустой строкой
- Выявляются ссылки и форматируются в соответствии с ТЗ
- Происходит очистка от html-тегов
- Весь текст разбивается по отдельным словам
- Происходит сборка текста в строки заданной длины

**Недостаток (возможно, не алгоритма, а ТЗ)**: длина некоторых ссылок превышает лимит в 80 символов.

**Решение:** можно реализовать замену "длинных" ссылок короткими через сервисы типа [https://clck.ru/](https://clck.ru/)

Дополнительно реализованы:
- Усложнение задачи 1: путь сохранения текстового файла парсится из URL (библиотека urlparse)
- Усложнение задачи 2: настройки забираюся из файла settings.ini (длина строки, рекомендации). Рекомендации реализованы в форме "черных" и "белых" списков источников.
- Для сайтов с динамически подгружаемым контентом реализовано получение содержимого через Selenium Webdriver (отдельное приложение)

#### Направления улучшение программы
1) Если предположить, что программа делается для широкого круга пользователей, то необходимо прикрутить хотя бы минималистичный GUI.
2) Улучшить обработку ошибок: не все участки кода работают безопасно.
3) ???????
4) [PROFIT!](https://lurkmore.to/%D0%9F%D0%A0%D0%9E%D0%A4%D0%98%D0%A2#.D0.98.D1.81.D0.BF.D0.BE.D0.BB.D1.8C.D0.B7.D0.BE.D0.B2.D0.B0.D0.BD.D0.B8.D0.B5)

### Файлы и папки
- **Задача C++ или Python - Mini readability - new (11).pdf** - тестовое задание
- **web_optimizerREQUESTS.py** - решение задачи, получающее данные через requests
- **web_optimizerSELEN.py** - решение задачи, получающее данные через Selenium WebDriver
- **urls for web_optimizerBS4.txt** - список статей, использованных для проверки алгоритма
- **settings.ini** - файл настроек
- **chromedriver.exe** - файл windows-драйвера для web_optimizerSELEN.py (версия 91.х)
- папки **zona.media, habr.com, meduza.io, tass.ru** - примеры работы приложения
- папка **dist** - исполняемые файлы **.exe**, файл настроек, файл логирования ошибок. **Внимание! web_optimizerSELEN.exe будет работать только после добавления chromedriver в PATH.
